{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMAIL spam detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports + python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import email\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "from plotly import tools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    Seq2SeqTrainer,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    ")\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitTrainer, SetFitModel\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    BaggingClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import scienceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Download glove embedding if it not exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python -m gensim.scripts.glove2word2vec --input glove.6B.300d.txt --output glove.6B.300d.word2vec.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Getting ham/spam sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamdata = glob.glob(\"ham/*\")\n",
    "spamdata = glob.glob(\"spam/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamdata[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamdata[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_content(email_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "\n",
    "    This function extracts the content from an email file located at the specified path. \n",
    "\n",
    "    # Args:\n",
    "\n",
    "    * `email_path (str)`: The path to the email file.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    * `str`: The extracted email content as a string.\n",
    "\n",
    "    Raises:\n",
    "\n",
    "    * `FileNotFoundError`: If the email file is not found at the given path.\n",
    "    * `IOError`: If there is an error reading the email file.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> content = get_email_content(\"/path/to/email.eml\")\\n\n",
    "    >>> print(content)\n",
    "    \"\"\"\n",
    "    file = open(email_path,encoding='latin1')\n",
    "    try:\n",
    "        msg = email.message_from_file(file)\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == 'text/plain':\n",
    "                return part.get_payload()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "def get_email_content_bulk(email_paths: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "\n",
    "    This function extracts the content from multiple email files specified in a list of paths.\n",
    "\n",
    "    # Args:\n",
    "\n",
    "    * `email_paths (list[str])`: A list of paths to email files.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    * `list[str]`: A list containing the extracted content of each email file. \n",
    "\n",
    "    Raises:\n",
    "\n",
    "    * `FileNotFoundError`: If any email file is not found at the given path.\n",
    "    * `IOError`: If there is an error reading any email file.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> email_paths = [\"/path/to/email1.eml\", \"/path/to/email2.eml\"]\\n\n",
    "    >>> contents = get_email_content_bulk(email_paths)\\n\n",
    "    >>> print(contents)\n",
    "    \"\"\"\n",
    "    email_contents = [get_email_content(o) for o in email_paths]\n",
    "    return email_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_path = [hamdata]\n",
    "spam_path = [spamdata]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting ham samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_sample = np.asarray([train_test_split(o, train_size=0.7, random_state=52, shuffle=True) for o in ham_path], dtype=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train = np.array([])\n",
    "ham_test = np.array([])\n",
    "for o in ham_sample:\n",
    "    ham_train = np.concatenate((ham_train,o[0]),axis=0)\n",
    "    ham_test = np.concatenate((ham_test,o[1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train.shape, ham_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting spam samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_sample = np.asarray([train_test_split(o) for o in spam_path], dtype = \"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train = np.array([])\n",
    "spam_test = np.array([])\n",
    "for o in spam_sample:\n",
    "    spam_train = np.concatenate((spam_train,o[0]),axis=0)\n",
    "    spam_test = np.concatenate((spam_test,o[1]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train.shape, spam_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating x_train, y_train (paths to containing texts), x_test, y_test (corresponding labels to contents in x_train y_train respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_train_label = [0]*ham_train.shape[0]\n",
    "spam_train_label = [1]*spam_train.shape[0]\n",
    "x_train = np.concatenate((ham_train,spam_train))\n",
    "y_train = np.concatenate((ham_train_label,spam_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_test_label = [0]*ham_test.shape[0]\n",
    "spam_test_label = [1]*spam_test.shape[0]\n",
    "x_test = np.concatenate((ham_test,spam_test))\n",
    "y_test = np.concatenate((ham_test_label,spam_test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle data we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shuffle_index = np.random.permutation(np.arange(0,x_train.shape[0]))\n",
    "test_shuffle_index = np.random.permutation(np.arange(0,x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shuffle_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shuffle_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[train_shuffle_index]\n",
    "y_train= y_train[train_shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[test_shuffle_index]\n",
    "y_test = y_test[test_shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting train and test texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_email_content_bulk(x_train)\n",
    "x_test = get_email_content_bulk(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing nullables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null(datas: list[str],labels: list[int]) -> tuple[list[str], list[int]]:\n",
    "    \"\"\"\n",
    "    Description: Removes elements from both lists where the corresponding label is None.\n",
    "\n",
    "    # Args:\n",
    "        datas (list[str]): A list of strings.\n",
    "        labels (list[int]): A list of integers with the same length as `datas`.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[str], list[int]]: A tuple containing the updated lists of strings and labels.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the lengths of `datas` and `labels` are not equal.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> datas = [\"apple\", \"banana\", None, \"orange\"]\n",
    "    >>> labels = [1, 1, 0, 1]\n",
    "    >>> remove_null(datas, labels)\n",
    "    (['apple', 'banana', 'orange'], [1, 1, 1])\n",
    "\"\"\"\n",
    "    not_null_idx = [i for i,o in enumerate(datas) if o is not None]\n",
    "    return np.array(datas)[not_null_idx],np.array(labels)[not_null_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = remove_null(x_train,y_train)\n",
    "x_test,y_test = remove_null(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cleaning up with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperlink(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Description: Removes hyperlinks from a given string.\n",
    "\n",
    "    # Args:\n",
    "        word (str): The string potentially containing hyperlinks.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with hyperlinks removed.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> remove_hyperlink(\"Check out https://www.example.com!\")\n",
    "    'Check out !'\n",
    "    \"\"\"\n",
    "    return  re.sub(r\"http\\S+\", \"\", word)\n",
    "\n",
    "def to_lower(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Description: Converts a string to lowercase.\n",
    "\n",
    "    # Args:\n",
    "        word (str): The string to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: The lowercase version of the string.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> to_lower(\"Hello WORLD\")\n",
    "    'hello world'\n",
    "    \"\"\"\n",
    "    result = word.lower()\n",
    "    return result\n",
    "\n",
    "def remove_number(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Description: Removes numbers from a string.\n",
    "\n",
    "    # Args:\n",
    "        word (str): The string potentially containing numbers.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with numbers removed.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> remove_number(\"This is year 2023\")\n",
    "    'This is year '\n",
    "    \"\"\"\n",
    "    result = re.sub(r'\\d+', '', word)\n",
    "    return result\n",
    "\n",
    "def remove_punctuation(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Description: Removes punctuation marks from a string.\n",
    "\n",
    "    # Args:\n",
    "        word (str): The string potentially containing punctuation.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with punctuation removed.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> remove_punctuation(\"Hello, world!\")\n",
    "    'Hello world'\n",
    "    \"\"\"\n",
    "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return result\n",
    "\n",
    "def remove_whitespace(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Description: Removes leading and trailing whitespaces from a string.\n",
    "\n",
    "    # Args:\n",
    "        word (str): The string potentially containing whitespaces.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with leading and trailing whitespaces removed.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> remove_whitespace(\"  hello   \")\n",
    "    'hello'\n",
    "    \"\"\"\n",
    "    result = word.strip()\n",
    "    return result\n",
    "\n",
    "def replace_newline(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Description: Replaces newline characters with spaces in a string.\n",
    "\n",
    "    # Args:\n",
    "        word (str): The string potentially containing newline characters.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with newline characters replaced by spaces.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> replace_newline(\"Hello\\nWorld\")\n",
    "    'Hello World'\n",
    "    \"\"\"\n",
    "    return word.replace('\\n','')\n",
    "\n",
    "def clean_up_pipeline(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Description: Applies a series of cleaning operations to a sentence.\\n\n",
    "    For now:\n",
    "    >>> remove_hyperlink()\n",
    "    >>> replace_newline()\n",
    "    >>> to_lower()\n",
    "    >>> remove_number()\n",
    "    >>> remove_punctuation()\n",
    "    >>> remove_whitespace()\n",
    "\n",
    "    # Args:\n",
    "        sentence (str): The sentence to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned sentence.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> clean_up_pipeline(\"  HeLlO, wOrLd! 2023  \")\n",
    "    'hello world'\n",
    "    \"\"\" \n",
    "    cleaning_utils = [remove_hyperlink,\n",
    "                      replace_newline,\n",
    "                      to_lower,\n",
    "                      remove_number,\n",
    "                      remove_punctuation,\n",
    "                      remove_whitespace]\n",
    "    for o in cleaning_utils:\n",
    "        sentence = o(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_up_pipeline(o) for o in x_train]\n",
    "x_test = [clean_up_pipeline(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [word_tokenize(o) for o in x_train]\n",
    "x_test = [word_tokenize(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Description: Removes stop words from a list of stop words from sklearn.\n",
    "\n",
    "    # Args:\n",
    "        words (list[str]): A list of words.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The list of words with stop words removed.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> remove_stop_words([\"the\", \"quick\", \"brown\", \"fox\"])\n",
    "    ['quick', 'brown', 'fox']\n",
    "    \"\"\"\n",
    "    result = [i for i in words if i not in sklearn_stop_words]\n",
    "    return result\n",
    "\n",
    "def word_stemmer(words: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Description: Applies stemming to a list of words.\n",
    "\n",
    "    # Args:\n",
    "        words (list[str]): A list of words.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The list of stemmed words.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> word_stemmer([\"running\", \"jumps\", \"jumped\"])\n",
    "    ['run', 'jump', 'jump']\n",
    "    \"\"\"\n",
    "    return [stemmer.stem(o) for o in words]\n",
    "\n",
    "def word_lemmatizer(words: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Description: Applies lemmatization to a list of words.\n",
    "\n",
    "    # Args:\n",
    "        words (list[str]): A list of words.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The list of lemmatized words.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> word_lemmatizer([\"running\", \"better\", \"best\"])\n",
    "    ['run', 'good', 'good'] \n",
    "    \"\"\"\n",
    "    return [lemmatizer.lemmatize(o) for o in words]\n",
    "\n",
    "def clean_token_pipeline(words: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Description: Applies a series of cleaning operations to a list of tokens.\n",
    "\n",
    "    # Args:\n",
    "        words (list[str]): A list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The cleaned list of tokens.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> clean_token_pipeline([\" The \", \"quick!\", \"ran.\", \"2023\"])\n",
    "    ['quick', 'ran']\n",
    "    \"\"\"\n",
    "    cleaning_utils = [remove_stop_words,\n",
    "                      word_lemmatizer]\n",
    "    for o in cleaning_utils:\n",
    "        words = o(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [clean_token_pipeline(o) for o in x_train]\n",
    "x_test = [clean_token_pipeline(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [\" \".join(o) for o in x_train]\n",
    "x_test = [\" \".join(o) for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(text: str, mask: str=None, max_words: int=200, max_font_size: int=100, figure_size: tuple[int,int]=(24.0,16.0), \n",
    "                   title: str = None, title_size: int=40, image_color: str=False) -> None:\n",
    "    \"\"\"\n",
    "    Description: Generates and displays a word cloud image.\n",
    "\n",
    "    # Args:\n",
    "        text (str): The text to generate the word cloud from.\n",
    "        mask (str, optional): Path to an image mask. Defaults to None.\n",
    "        max_words (int, optional): Maximum number of words in the cloud. Defaults to 200.\n",
    "        max_font_size (int, optional): Maximum font size for words. Defaults to 100.\n",
    "        figure_size (tuple[int,int], optional): Size of the figure. Defaults to (24.0,16.0).\n",
    "        title (str, optional): Title for the word cloud. Defaults to None.\n",
    "        title_size (int, optional): Font size of the title. Defaults to 40.\n",
    "        image_color (str, optional): If True, the image will be converted to grayscale. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> plot_wordcloud(\"This is an example text for word cloud generation\")\n",
    "    \"\"\"\n",
    "    stopwords = set(STOPWORDS)\n",
    "    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n",
    "    stopwords = stopwords.union(more_stopwords)\n",
    "\n",
    "    wordcloud = WordCloud(background_color='black',\n",
    "                    stopwords = stopwords,\n",
    "                    max_words = max_words,\n",
    "                    max_font_size = max_font_size, \n",
    "                    random_state = 42,\n",
    "                    width=800, \n",
    "                    height=400,\n",
    "                    mask = mask)\n",
    "    wordcloud.generate(str(text))\n",
    "    \n",
    "    plt.figure(figsize=figure_size)\n",
    "    if image_color:\n",
    "        image_colors = ImageColorGenerator(mask);\n",
    "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
    "        plt.title(title, fontdict={'size': title_size,  \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    else:\n",
    "        plt.imshow(wordcloud);\n",
    "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
    "                                  'verticalalignment': 'bottom'})\n",
    "    plt.axis('off');\n",
    "    plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train_index = [i for i,o in enumerate(y_train) if o == 1]\n",
    "non_spam_train_index = [i for i,o in enumerate(y_train) if o == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_train_index[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_spam_train_index[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_email = np.array(x_train)[spam_train_index]\n",
    "non_spam_email = np.array(x_train)[non_spam_train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(spam_email,title = 'Spam Email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(non_spam_email, title=\"Non spam email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom function for ngram generation ##\n",
    "def generate_ngrams(text: str, n_gram: int=1) -> list[str]:\n",
    "    \"\"\"\n",
    "    Description: Generates n-grams from a given text.\n",
    "\n",
    "    # Args:\n",
    "        text (str): The text to generate n-grams from.\n",
    "        n_gram (int, optional): The value of n for the n-grams. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of n-grams. \n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> generate_ngrams(\"This is a sentence\", 2)\n",
    "    ['This is', 'is a', 'a sentence'] \n",
    "    \"\"\"\n",
    "    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "    ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "## custom function for horizontal bar chart ##\n",
    "def horizontal_bar_chart(df: pd.DataFrame, color: str)-> go.Bar:\n",
    "    \"\"\"\n",
    "    Description: Creates a horizontal bar chart using plotly.\n",
    "\n",
    "    # Args:\n",
    "        df (pd.DataFrame): The dataframe containing the data.\n",
    "        color (str): The color for the bars.\n",
    "\n",
    "    Returns:\n",
    "        go.Bar: The plotly bar chart object.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> horizontal_bar_chart(data, \"blue\")\n",
    "    \"\"\"\n",
    "    trace = go.Bar(\n",
    "        y=df[\"word\"].values[::-1],\n",
    "        x=df[\"wordcount\"].values[::-1],\n",
    "        showlegend=False,\n",
    "        orientation = 'h',\n",
    "        marker=dict(\n",
    "            color=color,\n",
    "        ),\n",
    "    )\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_in_bar_chart(word_count: int=1) -> None:\n",
    "    \"\"\"\n",
    "    Description: Visualizes word counts in a bar chart.\n",
    "\n",
    "    # Args:\n",
    "        word_count (int, optional): The number of words to include in the chart. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        None \n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> visualize_in_bar_chart(20) \n",
    "    \"\"\"\n",
    "    ## Get the bar chart from sincere questions ##\n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in non_spam_email:\n",
    "        for word in generate_ngrams(sent,word_count):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace0 = horizontal_bar_chart(fd_sorted.head(20), 'blue')\n",
    "\n",
    "    ## Get the bar chart from insincere questions ##\n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in spam_email:\n",
    "        for word in generate_ngrams(sent,word_count):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace1 = horizontal_bar_chart(fd_sorted.head(20), 'blue')\n",
    "\n",
    "    # Creating two subplots\n",
    "    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n",
    "                              subplot_titles=[\"Frequent words of non spam email\", \n",
    "                                              \"Frequent words of spam email\"])\n",
    "    fig.append_trace(trace0, 1, 1)\n",
    "    fig.append_trace(trace1, 1, 2)\n",
    "    fig['layout'].update(height=600, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "    py.iplot(fig, filename='word-plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_in_bar_chart(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_size = len(spam_train_index)\n",
    "non_spam_size = len(non_spam_train_index)\n",
    "total_train_size = spam_size + non_spam_size\n",
    "\n",
    "trace = go.Bar(\n",
    "    x=[\"Spam\",\"Non Spam\"],\n",
    "    y=[spam_size, non_spam_size],\n",
    "    marker=dict(\n",
    "        color=[spam_size, non_spam_size],\n",
    "        colorscale = 'Picnic',\n",
    "        reversescale = True\n",
    "    ),\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Target Count for Train Data',\n",
    "    font=dict(size=18)\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"TargetCount\")\n",
    "\n",
    "## target distribution ##\n",
    "labels = (np.array([\"Spam\", \"Non Spam\"]))\n",
    "sizes = (np.array(([spam_size,non_spam_size]))/total_train_size*100)\n",
    "\n",
    "trace = go.Pie(labels=labels, values=sizes)\n",
    "layout = go.Layout(\n",
    "    title='Train Data distribution',\n",
    "    font=dict(size=18),\n",
    "    width=600,\n",
    "    height=600,\n",
    ")\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"usertype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_test_index = [i for i,o in enumerate(y_test) if o == 1]\n",
    "non_spam_test_index = [i for i,o in enumerate(y_test) if o == 0]\n",
    "\n",
    "spam_size = len(spam_test_index)\n",
    "non_spam_size = len(non_spam_test_index)\n",
    "total_test_size = spam_size + non_spam_size\n",
    "\n",
    "trace = go.Bar(\n",
    "    x=[\"Spam\",\"Non Spam\"],\n",
    "    y=[spam_size, non_spam_size],\n",
    "    marker=dict(\n",
    "        color=[spam_size, non_spam_size],\n",
    "        colorscale = 'Picnic',\n",
    "        reversescale = True\n",
    "    ),\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Target Count for Test Data',\n",
    "    font=dict(size=18)\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"TargetCount\")\n",
    "\n",
    "## target distribution ##\n",
    "labels = (np.array([\"Spam\", \"Non Spam\"]))\n",
    "sizes = (np.array(([spam_size,non_spam_size]))/total_train_size*100)\n",
    "\n",
    "trace = go.Pie(labels=labels, values=sizes)\n",
    "layout = go.Layout(\n",
    "    title='Test Data Distribution',\n",
    "    font=dict(size=18),\n",
    "    width=600,\n",
    "    height=600,\n",
    ")\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename=\"usertype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [o.split(\" \") for o in x_train]\n",
    "x_test = [o.split(\" \") for o in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {\"label\": y_train, \"text\": x_train}\n",
    "dict2 = {\"label\": y_test, \"text\": x_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame.from_dict(dict1)\n",
    "df2 = pd.DataFrame.from_dict(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results/asd/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Feature extraction defs (will be used in training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Description: \\n\n",
    "    (For LLM models)\\n\n",
    "    Tokenizes a dataset using a specified tokenizer.\\n\n",
    "    (Because different LLM models works well with specific tokenizer)\\n\n",
    "    Without well selected tokenizer models can work in unpredictable form...\\n\n",
    "    And can cause a lot of loss/ degradation of perfomance/ etc...\n",
    "\n",
    "    # Args:\n",
    "        dataset: The dataset to be tokenized.\n",
    "        tokenizer: The tokenizer object to use.\n",
    "\n",
    "    Returns:\n",
    "        The tokenized dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenization(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    def tokenization_t5(examples, padding=\"max_length\"):\n",
    "        \n",
    "        # Add T5 prefix to the text\n",
    "        text = [\"classify as ham or spam: \" +\n",
    "                item for item in examples[\"text\"]]\n",
    "\n",
    "        # Tokenize text and labels\n",
    "        inputs = tokenizer(text, max_length=tokenizer.model_max_length,\n",
    "                           padding=padding, truncation=True)\n",
    "        labels = tokenizer(\n",
    "            text_target=examples[\"label\"], max_length=max_label_length, padding=True, truncation=True)\n",
    "\n",
    "        # Replace tokenizer.pad_token_id in the labels by -100 to ignore padding in the loss\n",
    "        inputs[\"labels\"] = [\n",
    "            [(x if x != tokenizer.pad_token_id else -100) for x in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "        return inputs\n",
    "\n",
    "    if tokenizer is None:\n",
    "        return dataset\n",
    "\n",
    "    elif \"T5\" in type(tokenizer).__name__:\n",
    "        # Extra step to convert our 0/1 labels into \"ham\"/\"spam\" strings\n",
    "        dataset = dataset.map(\n",
    "            lambda x: {\"label\": \"ham\" if x[\"label\"] == 0 else \"spam\"})\n",
    "\n",
    "        # Calculate the max label length after tokenization\n",
    "        tokenized_label = dataset[\"train\"].map(\n",
    "            lambda x: tokenizer(x[\"label\"], truncation=True), batched=True)\n",
    "        max_label_length = max([len(x) for x in tokenized_label[\"input_ids\"]])\n",
    "\n",
    "        return dataset.map(tokenization_t5, batched=True, remove_columns=[\"label\"])\n",
    "\n",
    "    else:\n",
    "        return dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Training a lot of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Running baselines. You can add yours below\"\"\"\n",
    "MODELS = {\n",
    "    \"NB\": (MultinomialNB(), 1000),\n",
    "    \"LR\": (LogisticRegression(), 500),\n",
    "    \"KNN\": (KNeighborsClassifier(n_neighbors=1), 150),\n",
    "    \"SVM\": (SVC(kernel=\"sigmoid\", gamma=1.0), 3000),\n",
    "    \"XGBoost\": (XGBClassifier(learning_rate=0.01, n_estimators=150), 2000),\n",
    "    \"LightGBM\": (LGBMClassifier(learning_rate=0.01, num_leaves=20), 3000),\n",
    "    \"Catboost\": (CatBoostClassifier(learning_rate=0.01),2000),\n",
    "    \"RandomForestClassifier\": (RandomForestClassifier(n_estimators=50, criterion=\"gini\"),2000),\n",
    "    \"AdaBoostClassifier\": (AdaBoostClassifier(n_estimators=100,learning_rate=0.1),2000),\n",
    "    \"BaggingClassifier\": (BaggingClassifier(n_estimators=20),2000),\n",
    "    \"ExtraTreesClassifier\": (ExtraTreesClassifier(n_estimators=100),2000),\n",
    "    \"GradientBoostingClassifier\": (GradientBoostingClassifier(learning_rate=0.01),2000),\n",
    "    \"GaussianNB\": (GaussianNB(var_smoothing=1e-10), 2000),\n",
    "    \"BernoulliNB\": (BernoulliNB(alpha=0.1), 2000),\n",
    "    \"DecisionTreeClassifier\": (DecisionTreeClassifier(criterion=\"gini\", max_depth=20),2000),\n",
    "}\n",
    "\n",
    "\"\"\"SCORING parameters. You can add yours below (cohen kappa enjoyers=>)\"\"\"\n",
    "SCORING = {\n",
    "    \"f1\": f1_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"recall\": recall_score,\n",
    "    \"accuracy\": accuracy_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-related stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMS = {\n",
    "    \"RoBERTa\": (\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"roberta-base\", num_labels=2\n",
    "        ),\n",
    "        AutoTokenizer.from_pretrained(\"roberta-base\"),\n",
    "    ),\n",
    "    \"SetFit-mpnet\": (\n",
    "        SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\"),\n",
    "        None,\n",
    "    ),\n",
    "    \"FLAN-T5-base\": (\n",
    "        AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\"),\n",
    "        AutoTokenizer.from_pretrained(\"google/flan-t5-base\"),\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional functions (Utils):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df: pd.DataFrame, train_size=0.8, has_val: bool=True) -> tuple[tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame], datasets.DatasetDict]:\n",
    "    \"\"\"\n",
    "    Description: Splits a Pandas DataFrame into train, validation, and test sets. Also creates DatasetDict for training pytorch LLMs.\n",
    "\n",
    "    # Args:\n",
    "        df (pd.DataFrame): The DataFrame to split.\n",
    "        train_size (float, optional): The proportion of data for the training set. Defaults to 0.8.\n",
    "        has_val (bool, optional): Whether to create a validation set. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: Train, validation, and test DataFrames (if has_val is True).\n",
    "            - datasets.DatasetDict: Datasets for LLM training =/\n",
    "    \"\"\"\n",
    "    # Convert int train_size into float\n",
    "    if isinstance(train_size, int):\n",
    "        train_size = train_size / len(df)\n",
    "\n",
    "    # Shuffled train/val/test split\n",
    "    df = df.sample(frac=1, random_state=0)\n",
    "    df_train, df_test = train_test_split(\n",
    "        df, test_size=1 - train_size, stratify=df[\"label\"]\n",
    "    )\n",
    "\n",
    "    if has_val:\n",
    "        df_test, df_val = train_test_split(\n",
    "            df_test, test_size=0.5, stratify=df_test[\"label\"]\n",
    "        )\n",
    "        return (\n",
    "            (df_train, df_val, df_test),\n",
    "            datasets.DatasetDict(\n",
    "                {\n",
    "                    \"train\": datasets.Dataset.from_pandas(df_train),\n",
    "                    \"val\": datasets.Dataset.from_pandas(df_val),\n",
    "                    \"test\": datasets.Dataset.from_pandas(df_test),\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        return (\n",
    "            (df_train, df_test),\n",
    "            datasets.DatasetDict(\n",
    "                {\n",
    "                    \"train\": datasets.Dataset.from_pandas(df_train),\n",
    "                    \"test\": datasets.Dataset.from_pandas(df_test),\n",
    "                }\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed) -> None:\n",
    "    \"\"\"\n",
    "    Description: Sets the random seed for reproducibility.\n",
    "\n",
    "    # Args:\n",
    "        seed (int): The seed value to set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(task_name: str, experiment: str, dataset_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Description: Plots scores for a specific task, experiment, and dataset.\n",
    "\n",
    "    # Args:\n",
    "        task_name (str): The name of the task.\n",
    "        experiment (str): The name of the experiment.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    scores = pd.read_csv(f\"outputs/csv/{experiment}.csv\", index_col=0)\n",
    "\n",
    "    x = np.arange(len(scores))\n",
    "    width = 0.2\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    rects1 = ax.bar(x=x - width, height=scores[\"f1\"], width=width, label=\"F1 score\")\n",
    "    rects2 = ax.bar(x=x, height=scores[\"precision\"], width=width, label=\"Precision\")\n",
    "    rects3 = ax.bar(x=x + width, height=scores[\"recall\"], width=width, label=\"Recall\")\n",
    "\n",
    "    ax.set_title(f\"{dataset_name.upper()}\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_xticks(x, labels=scores.index, fontsize=6)\n",
    "    plt.legend(bbox_to_anchor=(0.5, -0.25), loc=\"lower center\", ncol=4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    Path(f\"outputs/pdf/\").mkdir(parents=True, exist_ok=True)\n",
    "    Path(f\"outputs/png/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    plt.savefig(f\"outputs/pdf/{experiment}.pdf\", format=\"pdf\")\n",
    "    plt.savefig(f\"outputs/png/{experiment}.png\", format=\"png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores(task:str, experiment: str, index: str, values: dict) -> None:\n",
    "    \"\"\"\n",
    "    Description: Saves scores for a task and experiment to a file.\\n\n",
    "    (P.S: Check llms and models lists and add corresponding to yours to save scores=))\n",
    "\n",
    "    # Args:\n",
    "        task (str): The name of the task.\n",
    "        experiment (str): The name of the experiment.\n",
    "        index (str): The index or identifier for the scores. \n",
    "        values (dict): A dictionary containing the scores to save.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\" \n",
    "    llms = [\n",
    "        \"BERT\",\n",
    "        \"RoBERTa\",\n",
    "        \"SetFit-MiniLM\",\n",
    "        \"SetFit-mpnet\",\n",
    "        \"FLAN-T5-small\",\n",
    "        \"FLAN-T5-base\",\n",
    "    ]\n",
    "    models = [\"NB\", \"LR\", \"KNN\", \"SVM\", \"XGBoost\", \"LightGBM\", \"Catboost\",\n",
    "        \"RandomForestClassifier\", \"AdaBoostClassifier\", \"BaggingClassifier\", \"ExtraTreesClassifier\",\n",
    "        \"GradientBoostingClassifier\",\"GaussianNB\",\"BernoulliNB\",\"DecisionTreeClassifier\"]\n",
    "\n",
    "    Path(f\"outputs/csv/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file = Path(f\"outputs/csv/{experiment}.csv\")\n",
    "    if file.is_file():\n",
    "        scores = pd.read_csv(f\"outputs/csv/{experiment}.csv\", index_col=0)\n",
    "        scores.loc[index] = values\n",
    "    else:\n",
    "        if index in llms:\n",
    "            scores = pd.DataFrame(\n",
    "                index=llms,\n",
    "                columns=list(SCORING.keys()) + [\"training_time\", \"inference_time\"],\n",
    "            )\n",
    "        else:\n",
    "            scores = pd.DataFrame(\n",
    "                index=models,\n",
    "                columns=list(SCORING.keys()) + [\"training_time\", \"inference_time\"],\n",
    "            )\n",
    "        scores.loc[index] = values\n",
    "\n",
    "    scores.to_csv(f\"outputs/csv/{experiment}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(experiment: str, dataset_name: str, model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Description: Plots the LLM loss curve for a specific experiment, dataset, and model.\n",
    "\n",
    "    # Args:\n",
    "        experiment (str): The name of the experiment.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        model_name (str): The name of the model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\" \n",
    "    log = pd.read_csv(f\"outputs/csv/loss_{model_name}_{experiment}.csv\")\n",
    "    log = pd.DataFrame(log).iloc[:-1]\n",
    "\n",
    "    train_losses = log[\"train_loss\"].dropna().values\n",
    "    eval_losses = log[\"eval_loss\"].dropna().values\n",
    "    x = np.arange(1, len(train_losses) + 1, step=1)\n",
    "\n",
    "    with plt.style.context([\"science\", \"high-vis\"]):\n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(x, train_losses, label=\"Training loss\")\n",
    "        plt.plot(x, eval_losses, label=\"Evaluation loss\")\n",
    "\n",
    "        ax.set_title(f\"{model_name} ({dataset_name})\")\n",
    "        ax.set_xticks(x, labels=range(1, len(x) + 1))\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "        Path(f\"outputs/pdf/\").mkdir(parents=True, exist_ok=True)\n",
    "        Path(f\"outputs/png/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        plt.savefig(f\"outputs/pdf/loss_{model_name}_{experiment}.pdf\", format=\"pdf\")\n",
    "        plt.savefig(\n",
    "            f\"outputs/png/loss_{model_name}_{experiment}.png\", format=\"png\", dpi=300\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description: Retrieves a dataset by name.\n",
    "\n",
    "    # Args:\n",
    "        name (str): The name of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe containing the dataset.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(f\"results/{name}/data.csv\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df(df: pd.DataFrame, encoder=None)-> tuple[list[int], list[int], any|None]:\n",
    "    \"\"\"\n",
    "    Description: Encodes a dataframe using a provided encoder or infers the encoding.\n",
    "\n",
    "    # Args:\n",
    "        df (pd.DataFrame): The dataframe to encode.\n",
    "        encoder (optional): The encoder to use. If None, infers the encoding.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The encoded dataframe.\n",
    "    \"\"\"\n",
    "    if hasattr(encoder, \"vocabulary_\"):\n",
    "        X = encoder.transform(df[\"text\"]).toarray()\n",
    "    else:\n",
    "        X = encoder.fit_transform(df[\"text\"]).toarray()\n",
    "    y = df[\"label\"].values\n",
    "    return X, y, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baselines(seeds: list[int], datasets: list[str], task_name: str, test_set: str=\"test\") -> None:\n",
    "    \"\"\"\n",
    "    Description: Trains baseline models for a given task and set of datasets.\n",
    "\n",
    "    # Args:\n",
    "        seeds (list): A list of seed values for reproducibility. \n",
    "        datasets (list): A list of dataset names. \n",
    "        task_name (str): The name of the task. \n",
    "        test_set (str, optional): val for cv. test for evaluating on test set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    ## Example:\n",
    "    >>> train_baselines(\n",
    "        list(range(1)),\n",
    "        datasets=[\"asd\"],\n",
    "        task_name=\"TrainBaselines\",\n",
    "        test_set = \"test\")\n",
    "    \"\"\"\n",
    "    for seed in list(seeds):\n",
    "        set_seed(seed)\n",
    "    \n",
    "        for dataset_name in list(datasets):\n",
    "            # Create list of metrics\n",
    "            scores = pd.DataFrame(\n",
    "                index=list(MODELS.keys()),\n",
    "                columns=list(SCORING.keys()) + [\"training_time\", \"inference_time\"],\n",
    "            )\n",
    "\n",
    "            df = get_dataset(dataset_name)\n",
    "            (df_train, df_val, df_test), _ = train_val_test_split(\n",
    "                    df, train_size=0.7, has_val=True\n",
    "            )\n",
    "                # Name experiment\n",
    "            experiment = (\n",
    "                f\"ml_{test_set}_{task_name}_train_seed_{seed}\"\n",
    "            )\n",
    "\n",
    "            # Cross-validate and test every model\n",
    "            for model_name, (model, max_iter) in MODELS.items():\n",
    "                encoder = TfidfVectorizer(max_features=max_iter)\n",
    "                x_train, y_train, encoder = encode_df(df_train, encoder)\n",
    "                x_test, y_test, encoder = encode_df(df_test, encoder)\n",
    "                # Evaluate model with cross-validation\n",
    "                if test_set == \"val\":\n",
    "                    cv = cross_validate(\n",
    "                        model,\n",
    "                        x_train,\n",
    "                        y_train,\n",
    "                        scoring=list(SCORING.keys()),\n",
    "                        cv=5,\n",
    "                        n_jobs=-1,\n",
    "                    )\n",
    "                    for score_name, score_fn in SCORING.items():\n",
    "                        scores.loc[score_name, model_name] = cv[\n",
    "                            f\"test_{score_name}\"\n",
    "                        ].mean()\n",
    "\n",
    "\n",
    "                # Evaluate model on test set\n",
    "                if test_set == \"test\":\n",
    "                    start = time.time()\n",
    "                    model.fit(x_train, y_train)\n",
    "                    end = time.time()\n",
    "                    scores.loc[model_name, \"training_time\"] = end - start\n",
    "\n",
    "                    start = time.time()\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    end = time.time()\n",
    "\n",
    "                    scores.loc[model_name, \"inference_time\"] = end - start\n",
    "                    for score_name, score_fn in SCORING.items():\n",
    "                        scores.loc[model_name, score_name] = score_fn(\n",
    "                            y_pred, y_test\n",
    "                        )\n",
    "\n",
    "                save_scores(\n",
    "                    task_name, experiment, model_name, scores.loc[model_name].to_dict()\n",
    "                )\n",
    "\n",
    "            # Display scores\n",
    "            plot_scores(task_name, experiment, \"results\")\n",
    "            print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_baselines(\n",
    "    list(range(1)),\n",
    "    datasets=[\"asd\"],\n",
    "    task_name=\"TrainBaselines\",\n",
    "    test_set = \"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LLMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalOnTrainCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to evaluate on the training set during training.\"\"\"\n",
    "\n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_train = copy.deepcopy(control)\n",
    "            self._trainer.evaluate(\n",
    "                eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\"\n",
    "            )\n",
    "            return control_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(model, traindict, testdict, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Description: Creates a Hugging Face Trainer object for a given model and datasets.\n",
    "\n",
    "    # Args:\n",
    "        model: The Hugging Face model to train.\n",
    "        traindict: A Hugging Face DatasetDict containing the training data.\n",
    "        testdict: A Hugging Face DatasetDict containing the test data.\n",
    "        tokenizer (optional): The tokenizer associated with the model.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: A Hugging Face Trainer object configured for training.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> trainer = get_trainer(model, train_dataset, test_dataset, tokenizer)\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_metrics(y_pred):\n",
    "        \"\"\"Computer metrics during training.\"\"\"\n",
    "        logits, labels = y_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return evaluate.load(\"f1\").compute(\n",
    "            predictions=predictions, references=labels, average=\"macro\"\n",
    "        )\n",
    "\n",
    "    if type(model).__name__ == \"SetFitModel\":\n",
    "        trainer = SetFitTrainer(\n",
    "            model=model,\n",
    "            train_dataset=traindict,\n",
    "            eval_dataset=testdict,\n",
    "            loss_class=CosineSimilarityLoss,\n",
    "            metric=\"f1\",\n",
    "            batch_size=4,\n",
    "            num_iterations=1,\n",
    "            num_epochs=1,\n",
    "        )\n",
    "        return trainer\n",
    "\n",
    "    elif \"T5\" in type(model).__name__ or \"FLAN\" in type(model).__name__:\n",
    "\n",
    "        def compute_metrics_t5(y_pred, verbose=0):\n",
    "            \"\"\"Computer metrics during training for T5-like models.\"\"\"\n",
    "            predictions, labels = y_pred\n",
    "\n",
    "            predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "            # Replace -100 with pad_token_id to decode them\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            predictions = [\n",
    "                1 if \"spam\" in predictions[i] else 0 for i in range(len(predictions))\n",
    "            ]\n",
    "            labels = [1 if \"spam\" in labels[i] else 0 for i in range(len(labels))]\n",
    "\n",
    "            result = evaluate.load(\"f1\").compute(\n",
    "                predictions=predictions, references=labels, average=\"macro\"\n",
    "            )\n",
    "            return result\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer, model=model, label_pad_token_id=-100, pad_to_multiple_of=8\n",
    "        )\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=\"experiments\",\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            learning_rate=1,\n",
    "            num_train_epochs=1,\n",
    "            predict_with_generate=True,\n",
    "            fp16=False,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=2,\n",
    "        )\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=traindict,\n",
    "            eval_dataset=testdict,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics_t5,\n",
    "        )\n",
    "        trainer.add_callback(EvalOnTrainCallback(trainer))\n",
    "        return trainer\n",
    "\n",
    "    else:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"experiments\",\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=4,\n",
    "            learning_rate=1,\n",
    "            num_train_epochs=1,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=1,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=traindict,\n",
    "            eval_dataset=testdict,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        trainer.add_callback(EvalOnTrainCallback(trainer))\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(trainer: transformers, model, dataset, tokenizer=None)-> list:\n",
    "    \"\"\"\n",
    "    Description: Generates predictions on a given dataset using a trained model and trainer.\n",
    "\n",
    "    # Args:\n",
    "        trainer: The Hugging Face Trainer object used for training.\n",
    "        model: The trained Hugging Face model.\n",
    "        dataset: A Dataset containing the data for prediction.\n",
    "        tokenizer (optional): The tokenizer associated with the model.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predictions for each example in the dataset.\n",
    "\n",
    "    # Example:\n",
    "\n",
    "    >>> predictions = predict(trainer, model, test_dataset, tokenizer)\n",
    "    \"\"\"\n",
    "    if type(model).__name__ == \"SetFitModel\":\n",
    "        return model(dataset[\"text\"])\n",
    "\n",
    "    elif \"T5\" in type(model).__name__:\n",
    "        predictions = trainer.predict(dataset)\n",
    "        predictions = tokenizer.batch_decode(\n",
    "            predictions.predictions, skip_special_tokens=True\n",
    "        )\n",
    "        predictions = [\n",
    "            \"ham\" if 0 in predictions[i] else \"spam\" for i in range(len(predictions))\n",
    "        ]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    else:\n",
    "        return trainer.predict(dataset).predictions.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_llms(seeds, datasets, task_name, test_set=\"test\"):\n",
    "    \"\"\"Train all the large language models.\"\"\"\n",
    "    for seed in list(seeds):\n",
    "        set_seed(seed)\n",
    "\n",
    "        for dataset_name in datasets:\n",
    "            print(dataset_name)\n",
    "            # Get metrics\n",
    "            scores = pd.DataFrame(\n",
    "                index=list(LLMS.keys()),\n",
    "                columns=list(SCORING.keys()) + [\"training_time\", \"inference_time\"],\n",
    "            )\n",
    "\n",
    "            df = get_dataset(dataset_name)\n",
    "            _, dataset = train_val_test_split(\n",
    "                    df, train_size=0.7, has_val=True\n",
    "                )\n",
    "            print(\"split succeded=)\")\n",
    "    \n",
    "            # Name experiment\n",
    "            experiment = (\n",
    "                f\"llm_{test_set}_train_seed_{seed}\"\n",
    "            )\n",
    "    \n",
    "            # Train, evaluate, test\n",
    "            for model_name, (model, tokenizer) in LLMS.items():\n",
    "                tokenized_dataset = tokenize(dataset, tokenizer)\n",
    "                trainer = get_trainer(model, tokenized_dataset['train'], tokenized_dataset['val'], tokenizer)\n",
    "                print(trainer.args)\n",
    "                # Train model\n",
    "                start = time.time()\n",
    "                print(\"beginning of train\")\n",
    "                train_result = trainer.train()\n",
    "                end = time.time()\n",
    "                print(f\"train ended after: {end}\")\n",
    "                scores.loc[model_name, \"training_time\"] = end - start\n",
    "                if \"SetFit\" not in model_name:\n",
    "                    log = pd.DataFrame(trainer.state.log_history)\n",
    "                    log.to_csv(f\"outputs/csv/loss_{model_name}_{experiment}.csv\")\n",
    "                    plot_loss(experiment, \"EMAIL spam check\", model_name)\n",
    "    \n",
    "                # Test model\n",
    "                start = time.time()\n",
    "                predictions = predict(\n",
    "                    trainer, model, tokenized_dataset[test_set], tokenizer\n",
    "                )\n",
    "                end = time.time()\n",
    "    \n",
    "                for score_name, score_fn in SCORING.items():\n",
    "                    scores.loc[model_name][score_name] = score_fn(\n",
    "                        dataset[test_set][\"label\"], predictions\n",
    "                    )\n",
    "    \n",
    "                scores.loc[model_name][\"inference_time\"] = end - start\n",
    "                save_scores(\n",
    "                    task_name, experiment, model_name, scores.loc[model_name].to_dict()\n",
    "                )\n",
    "    \n",
    "            # Display scores\n",
    "            plot_scores(task_name, experiment, task_name)\n",
    "            print(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_llms(\n",
    "    list(range(1)),\n",
    "    datasets=[\"asd\"],\n",
    "    task_name=\"LLMTrainCheck\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
